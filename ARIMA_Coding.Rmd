---
title: "ARIMA Coding Tutorial"
author: "Steve Midway"
date: "Spring 2020"
output:
  github_document:
    pandoc_args: --webtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

As with most things in R, there is a rich environments of packages and functions beyond the basics presented here. This coding tutorial is not intended to be comprehensive, but to simply serve as a way to get you coding ARIMA models and understanding their mechanics. This document is intended for you to work through. Much of the code is there, but much of it will need to be built on. 

### Libraries
```{r}
library(astsa) # Applied Statistical Time Series Analysis
```

### Time Series Objects
When working with time series data in R, we typically want to have the observations in a time series object, or `ts()`. For example, check out the dataset `Nile`.

```{r}
Nile
```

`Nile` is small dataset of annual flow of the Nile River (for more details, see `?Nile`). For our purposes, you can see here that `Nile` is listed as a Time Series object, and that can be further supported with the command `class(Nile)`, where `ts` means time series. You can certainly use data that are not a time series object, but there may be benefits to having data as a time series object and learning the capabailities of the time series object. For example, if we simply plot the `Nile` object, it will render as we expect (which is not necessarily the same as if the data were just a vector.)

```{r}
plot(Nile, las=1)
```
 
You may also want to convert your data to a time series object, which is accomplished by wrapping your data in `ts()`. It might be good to quickly read up on `ts` objects as they can be very useful for time series data. 

### Basic Time Series Example
Let's stick with the `Nile` data to get a feel for some basic functions we may want to use with a time series object. 

We may want to see the PACF and ACF plots. The `astsa::acf2` function will help us with this. Note that there are other packages and functions for producing these plots, but I will share some functions from the `astsa` package here. 

```{r}
acf2(Nile)
```

We might guess that with the ACF tailing off and the PACF cutting off at 1, that the an MA(1) model would be a good place to start. Let's try it. We can use `astsa::sarima` for the model fitting; this function is good for ARIMA fitting and produces useful diagnostic plots with the model output. 

```{r}
sarima(Nile, p = 0, d = 0, q = 1)
```

This is not a good fit. Some of the diagnostics look ok, but the Ljung-Box statistic is not acceptable (we want nearly all or all of the points above the blue line). What are some things we can try to have a more confident model fit? 

```{r, echo=F, eval=F}
Here, the things to evaluate are differencing the data, log transforming to improve error, using an AR order, etc. If multiple acceptable model structures are identified, use AIC to select. 
```

### Simulating and Recovering Estimates
A powerful appraoch to understanding models is to know the data you are putting into them. Then, if a model returns your known data (in the form of estimates), you can build confidence in how the model operates. If the model does not return what you expect based on the data you used, then you know that it will take some effort to diagnose what is happening that is not yet understood (which could be a model misspecification, a changing of scales, etc.). Let's use the function `arima.sim()` to simulate some data sets with known characteristics that we can then attempt to recover by modeling. You may want to review the help page for `arima.sim`, although it is pretty straightforward. 

Let's start with an AR(2) model and simulate some data for it. After you simulate this data, feel free to pause, evaluate the object you have created, and get comfortable with it. Note that in the function, you need to include the correct number of coefficients to match the order. In the example below, I specificed an AR(2), so I need two coefficients (i.e., `ar = c(1.5, -0.75)`). 

```{r}
x.ar2 <- arima.sim(model = list(order = c(2, 0, 0), ar = c(1.5, -0.75)), n = 200)
```

Now let's plot our data. 

```{r}
plot(x.ar2)
```

Next we might want to see some correlation plots.

```{r}
acf2(x.ar2)
```

Hopefully you can start to see how this data might be AR(2) (ignoring the fact that you know it was randomly generated to characterize AR(2) data!). Now we can simply fit an AR(2) model and see what the model fits look like. 

```{r}
sarima(x.ar2, p = 2, d = 0, q = 0)
```

Looks good! Prentend that you really didn't know the model order and the correlation plots were not that helpful. Noodle around with some other lower order ARMA models and see how the fits vary. 

```{r, echo=F, eval=F}
Examples: 
sarima(x, 1, 0, 0)
sarima(x, 1, 0, 1)
sarima(x, 2, 0, 1)
sarima(x, 2, 0, 2)
sarima(x, 1, 0, 2)
```

For the next part, repeat the structure of the exercise above (i.e., simulate data and then fit models to recover the estimates), but do so with an MA(1) model. This should be just as simple as the AR example above, but in this case, experiment with the coefficient, which can be either positive or negative, and of different magnitudes. How well does the model recover the coefficient? How does changing sample size effect the coefficient estimate? (Note that with increasing ARMA model complexity, some combinations of coefficients will not work and will result in warnings or errors. You won't likely encounter that in a simple model with one coefficient, but it may eventually happen as your models grow.)

### Forecasting

The `astsa` package has a nice function for forcasting, which is another important area of time series models. The function `sarima.for()` handles forecasting. Let's go back to our AR(2) data and model (in `ts` object `x`) and see what it does in a forecast. The `n.ahead = ` arguement is where you specify how many time points you want forecasted. 

```{r}
sarima.for(xdata = x.ar2, n.ahead = 3, p = 2, d = 0, q = 0)
```

Play around with the number of forecasted points. What happens as you increase it? If you have different data (in either model order and/or coefficients), does the forecasting and associated forecast uncertainty change? 


### Model Selection
You will likely want to use AIC (or comparable) for selecting ARIMA model orders. This can be done manually by fitting candidate models and comparing AICs. However, you might prefer an automated version of this information. The function `ar()` (in the built-in `stats` package) does this for AR models, but I am not sure how MA components are handled. Another option is to try `forecast::auto.arima`. Let's simulate some data and then see how well model selection performs. (Don't be afraid to read the help file on `auto.arima` as there is a lot of fine tuning you can do.)

```{r message=FALSE}
# Generate some data
x.ms <- arima.sim(model = list(order = c(0, 0, 2), ma = c(1.5, -0.75)), n = 200)

# Model selection
library(forecast)
auto.arima(x.ms)
```

Feel free to change the simulated data and see how the model selection works on models of different complexity. You might notice that the `auto.arima` function tends to select more complex models than you know to be true (based on the data generation). On one hand, it could be that a more complex model fits the data better than the parameters under which the data were generated. Also, it could be that AIC is tending toward more complex models. I do not know enough about AIC applied to time series data to be sure, but AIC in other applications can often tend toward the more complex side of the possible models. What can you modify in the `auto.arima` function to limit model complexity? What model order was selected for the `Nile` dataset?

### Coefficients
The last thing that we will play with in time series models is adding coefficients. For most time series models, the primary independent variable of interest is *time*. So for many time series models you may only want to model the effect of time. But there will likely be cases where you want to quantify the effect of some other indepednent variable along with time. Let's simulate some ARMA data and then a correlated and uncorrelated covariate to understand how to include covariates,

```{r}
# Generate some data
x.cor <- arima.sim(model = list(order = c(2, 0, 0), ar = c(1.5, -0.75)), n = 200)

# Correlated covariate
cov1 <- (x.cor * 1.5) + rnorm(n = length(x.cor))
# plot(x.cor ~ cov1) # Check relationship

# Uncorrelated covariate (cov2)
cov2 <- runif(n = length(x.cor),min = 0, max = 10)
# plot(x.cor ~ cov2) # Check relationship
```

Now let's run the ARIMA models and see what we recover. First the regression with the correlated predictor

```{r}
# ARIMA model with correlated covariate
sarima(x.cor, p = 2, d = 0, q = 0, xreg = cov1)
```

The *p*-value on `xreg` is reported as 0, suggesting that `cov1` provides a significant effect, which is esimated to be 0.604. 

Now with the uncorrelated predictor.

```{r}
# ARIMA model with uncorrelated covariate
sarima(x.cor, p = 2, d = 0, q = 0, xreg = cov2)
```

The *p*-value on `xreg` is large, suggesting that `cov2` provides no real effect, which is esimated to be 0.0063. 

I strongly suggest you consider more information than simply *p*-values when considering what covariates to include in your model and when assessing their performance; however, for the purposes of this tutorial, they illustrate the points we are trying to make about understanding if correlated and uncorrelated predictors are doing what we expect in a model. 

### Alternative Functions
As we have mentioned, there are many time series-related functions and many options for coding an ARIMA model. We have used functions from the `astsa` package here, but recall that in the `stats` library there are time series functions. Finally, although I am not sure about the limitations of this approach, you can use the `nlme::gls` function, which is for generalized least squares estimation, in which you can modify the `correlation =` argument to specify for an ARMA model. 

To illustrate, return to the `Nile` data set. If we want to model the annual river flow simply as a function of time, we code the model in what looks like an "intercept-only" formula, `y ~ 1`. Then, we would need to know the ARMA structure, but if we suppose that it is an AR(1) model, we might code it as below. How does this approach compare to other functions you have tried? 

```{r, message=FALSE}
library(nlme) # Needed for gls()
gls.ts <- gls(Nile ~ 1, correlation = corARMA(value = 0.25, p = 1))
summary(gls.ts)
```

